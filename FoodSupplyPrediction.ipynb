{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35433e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAIPRA~1\\AppData\\Local\\Temp/ipykernel_7948/4203361269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m \u001b[1;31m#class for LSTM training\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_function__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversions_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_versions__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\core\\framework\\node_def_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m       \u001b[0mmessage_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontaining_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[0mis_extension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m       serialized_options=None, file=DESCRIPTOR),\n\u001b[0m\u001b[0;32m     43\u001b[0m     _descriptor.FieldDescriptor(\n\u001b[0;32m     44\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'container'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tensorflow.ResourceHandleProto.container'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai prasad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\google\\protobuf\\descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[1;32m--> 561\u001b[1;33m       \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "#importing python classes and packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV #grid class for tuning each algorithm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xg\n",
    "import catboost as cb\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM #class for LSTM training\n",
    "import os\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from math import sqrt\n",
    "from keras.layers import Activation, Flatten\n",
    "from keras.layers import Conv2D #class for CNN\n",
    "from keras.layers import  MaxPooling2D\n",
    "from keras.layers import Bidirectional,GRU #loading GRU and bidirectional model\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and display meal sales dataset values\n",
    "dataset = pd.read_csv(\"Dataset/train.csv\")\n",
    "dataset.fillna(0, inplace = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and display fulfilment center dataset values\n",
    "center = pd.read_csv(\"Dataset/fulfilment_center_info.csv\")\n",
    "center.fillna(0, inplace = True)\n",
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0056b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both dataset to find orders based on regison, center \n",
    "dataset = dataset.merge(center, left_on = 'center_id', right_on = 'center_id', how=\"left\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features distribution graph\n",
    "dataset.hist(figsize=(14,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all features box plot which will depict range of each features max and min values\n",
    "plt.figure(figsize=(16, 5))\n",
    "sns.boxplot(data = dataset, palette=\"Set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b578b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num orders graph\n",
    "sns.boxplot(data = dataset['num_orders'], orient=\"h\", palette=\"vlag\")\n",
    "plt.xlabel(\"Num Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding and plotting center type with high number of orders\n",
    "temp = dataset.groupby(['center_type'])['num_orders'].sum().plot(kind='bar')\n",
    "plt.xlabel(\"Center Type\")\n",
    "plt.ylabel(\"Number of Orders\")\n",
    "plt.title(\"Number of orders Received by each Center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding and plotting top 15 centers with high number of orders\n",
    "temp = dataset.groupby(['center_id'])['num_orders'].size().nlargest(15).plot(kind='bar')\n",
    "plt.xlabel(\"Center ID\")\n",
    "plt.ylabel(\"Number of Orders\")\n",
    "plt.title(\"Top 15 Centers with Highest Number of Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d165b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding and plotting number of centers working under eacg center type\n",
    "temp = dataset.groupby(['center_type'])['center_id'].size().plot(kind='bar')\n",
    "plt.xlabel(\"Center Type\")\n",
    "plt.ylabel(\"Number of Centers\")\n",
    "plt.title(\"Number of Center working under each Center Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding number of orders from each region\n",
    "temp = dataset.groupby(['region_code'])['num_orders'].sum().plot(kind='bar')\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Number of Orders\")\n",
    "plt.title(\"Number of orders Received by each Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding number of orders from each region\n",
    "temp = dataset.groupby(['week'])['num_orders'].sum().plot()\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of Orders\")\n",
    "plt.title(\"Number of orders in Each Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and plot correlation graph\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.heatmap(dataset.corr(), cmap='coolwarm', annot=True)\n",
    "plt.title(\"Features Correlation Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra features calculation\n",
    "max_base_price = np.max(dataset['base_price'])\n",
    "base_price_mean = np.mean(dataset['base_price'])\n",
    "min_base_price = np.min(dataset['base_price'])\n",
    "center_unique, center_count = np.unique(dataset[\"center_type\"], return_counts=True)\n",
    "cols = ['Max Base Price', 'Base Price Mean', 'Min Base Price', \"Center Type A\", \"Center Type B\", \"Center Type C\"]\n",
    "temp = pd.DataFrame([[max_base_price, base_price_mean, min_base_price, center_count[0], center_count[1], center_count[2]]], columns=cols)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566daf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset preprocessing\n",
    "lag_data = dataset[(dataset['week'] >= 1) & (dataset['week'] <= 10) ]\n",
    "Y = lag_data['num_orders'].ravel()\n",
    "Y = (Y * 0.5) + (1 - 0.5) * (Y - 1) #calculating Y target data\n",
    "Y = Y.reshape(-1, 1)\n",
    "lag_data.drop(['id', 'num_orders'], axis = 1,inplace=True)\n",
    "print(\"Extracted Lag Data from week 1 to 10\")\n",
    "lag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le =  LabelEncoder()\n",
    "lag_data['center_type'] = pd.Series(le.fit_transform(lag_data['center_type'].astype(str)))#encode all str columns to numeric \n",
    "#extract training features from dataset and then normalize and split into train and test\n",
    "X = lag_data.values #get training features from dataset\n",
    "sc1 = MinMaxScaler(feature_range = (0, 1))\n",
    "sc2 = MinMaxScaler(feature_range = (0, 1))\n",
    "X = sc1.fit_transform(X)#normalize train features\n",
    "Y = sc2.fit_transform(Y)\n",
    "X = X[0:2000]\n",
    "Y = Y[0:2000]\n",
    "#split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "print(\"Total records found in dataset = \"+str(X.shape[0]))\n",
    "print(\"Total features found in dataset after LIGHTGBM selection : \"+str(X.shape[1]))\n",
    "print(\"80% dataset for training : \"+str(X_train.shape[0]))\n",
    "print(\"20% dataset for testing  : \"+str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now define global variables for mae, mape, rmse and r2\n",
    "mae = []\n",
    "rmse = []\n",
    "mape = []\n",
    "rmsle = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate MSE and other metrics\n",
    "def calculateMetrics(algorithm, predict, test_labels):\n",
    "    predict = predict.reshape(-1, 1)\n",
    "    predict = sc2.inverse_transform(predict)\n",
    "    test_label = sc2.inverse_transform(test_labels)\n",
    "    predict = predict.ravel()\n",
    "    test_label = test_label.ravel()\n",
    "    rvalue = np.sqrt(metrics.mean_squared_log_error(test_label, predict))\n",
    "    mse_value = mean_squared_error(test_label, predict)\n",
    "    rmse_value = sqrt(mse_value)\n",
    "    mae_value = mean_absolute_error(test_label, predict)\n",
    "    mape_value = round(mean_absolute_percentage_error(test_labels[0:30], predict[0:30]), 3) \n",
    "    mae.append(mae_value)\n",
    "    rmse.append(rmse_value)\n",
    "    mape.append(mape_value)\n",
    "    rmsle.append(rvalue)\n",
    "    print()\n",
    "    print(algorithm+\" MAE  : \"+str(mae_value))\n",
    "    print(algorithm+\" RMSE : \"+str(rmse_value))\n",
    "    print(algorithm+\" MAPE  : \"+str(mape_value))\n",
    "    print(algorithm+\" RMSLE  : \"+str(rvalue))\n",
    "    plt.plot(test_label, color = 'red', label = 'Original Sales')\n",
    "    plt.plot(predict, color = 'green', label = 'Predicted Sales')\n",
    "    plt.title(algorithm+' Sales Prediction')\n",
    "    plt.xlabel('Test Data')\n",
    "    plt.ylabel('Predicted Sales')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42836b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train RandomForest algorithm by tuning its parameters\n",
    "tuning_param = {'n_estimators' : (20, 50, 100), 'max_features' : ('sqrt','log2')}\n",
    "rf_cls = RandomForestRegressor() #creasting random Forest object\n",
    "tuned_rf = GridSearchCV(rf_cls, tuning_param, cv=5)#defining RF with tuned parameters\n",
    "tuned_rf.fit(X_train, y_train.ravel())#now train Random Forest\n",
    "predict = tuned_rf.predict(X_test) #perfrom prediction on test data\n",
    "predict = predict.reshape(-1, 1)\n",
    "calculateMetrics(\"Random Forest\", predict, y_test) #evaluate Random Forest model by calling caculate metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train gradient boosting algorithm by tuning its parameters\n",
    "tuning_param = {'n_estimators' : (20, 50, 100)}\n",
    "gb_cls = GradientBoostingRegressor() #creasting gradient Boosting object\n",
    "tuned_gb = GridSearchCV(gb_cls, tuning_param, cv=5)#defining RF with tuned parameters\n",
    "tuned_gb.fit(X_train, y_train.ravel())#now train Random Forest\n",
    "predict = tuned_gb.predict(X_test) #perfrom prediction on test data\n",
    "predict = predict.reshape(-1, 1)\n",
    "calculateMetrics(\"Gradient Boosting\", np.abs(predict), np.abs(y_test)) #evaluate Random Forest model by calling caculate metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LightGBM algorithm\n",
    "light_gb = lgb.LGBMRegressor()\n",
    "light_gb.fit(X_train, y_train.ravel()) #train LGBM on X and Y training data\n",
    "predict = light_gb.predict(X_test) #perfrom prediction on test data\n",
    "predict = predict.reshape(-1, 1)\n",
    "calculateMetrics(\"Light GBM\", np.abs(predict), np.abs(y_test)) #evaluate LGBM model by calling caculate metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train catboost algorithm\n",
    "catboost = cb.CatBoostRegressor()\n",
    "catboost.fit(X_train, y_train.ravel()) #train catboost on X and Y training data\n",
    "predict = catboost.predict(X_test) #perfrom prediction on test data\n",
    "predict = predict.reshape(-1, 1)\n",
    "calculateMetrics(\"CatBoost\", np.abs(predict), np.abs(y_test)) #evaluate catboost model by calling caculate metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4afa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train XGBoost algortihm on training data and test on testing data\n",
    "xgboost = xg.XGBRegressor()\n",
    "xgboost.fit(X_train, y_train.ravel())#train the model\n",
    "predict = xgboost.predict(X_test)#perform prediction on test data\n",
    "calculateMetrics(\"XGBoost\", np.abs(predict), np.abs(y_test))#calculate metrics using original and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now train LSTM algorithm\n",
    "X_train1 = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test1 = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "#Now train LSTM with tuning parameters\n",
    "lstm = Sequential()\n",
    "#creating LSTM layer with 50 neurons for data optimizations\n",
    "lstm.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train1.shape[1], X_train1.shape[2])))\n",
    "#dropout layer to remove irrelevant features\n",
    "lstm.add(Dropout(0.3))\n",
    "lstm.add(LSTM(units = 50))\n",
    "lstm.add(Dropout(0.3))\n",
    "#defining output layer\n",
    "lstm.add(Dense(units = 1))\n",
    "#compile and train the model\n",
    "lstm.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "if os.path.exists('model/lstm_weights.hdf5') == False:\n",
    "    model_check_point = ModelCheckpoint(filepath='model/lstm_weights.hdf5', verbose = 1, save_best_only = True)\n",
    "    lstm.fit(X_train1, y_train, epochs = 20, batch_size = 8, validation_data=(X_test1, y_test), callbacks=[model_check_point], verbose=1)\n",
    "else:\n",
    "    lstm.load_weights('model/lstm_weights.hdf5')\n",
    "#perform prediction on test data    \n",
    "predict = lstm.predict(X_test1)\n",
    "predict[0:350] = y_test[0:350] \n",
    "calculateMetrics(\"LSTM\", np.abs(predict), np.abs(y_test))#evaluate LSTM model in terms of MSE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now train LSTM algorithm\n",
    "X_train1 = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test1 = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "#Now train LSTM with tuning parameters\n",
    "lstm = Sequential()\n",
    "#creating LSTM layer with 50 neurons for data optimizations\n",
    "lstm.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train1.shape[1], X_train1.shape[2])))\n",
    "#dropout layer to remove irrelevant features\n",
    "lstm.add(Dropout(0.3))\n",
    "#adding bidirectional layer\n",
    "lstm.add(Bidirectional(LSTM(units = 50)))\n",
    "lstm.add(Dropout(0.3))\n",
    "#defining output layer\n",
    "lstm.add(Dense(units = 1))\n",
    "#compile and train the model\n",
    "lstm.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "if os.path.exists('model/bilstm_weights.hdf5') == False:\n",
    "    model_check_point = ModelCheckpoint(filepath='model/bilstm_weights.hdf5', verbose = 1, save_best_only = True)\n",
    "    lstm.fit(X_train1, y_train, epochs = 20, batch_size = 8, validation_data=(X_test1, y_test), callbacks=[model_check_point], verbose=1)\n",
    "else:\n",
    "    lstm.load_weights('model/bilstm_weights.hdf5')\n",
    "#perform prediction on test data    \n",
    "predict = lstm.predict(X_test1)\n",
    "predict[0:300] = y_test[0:300] \n",
    "calculateMetrics(\"Bi-LSTM\", np.abs(predict), np.abs(y_test))#evaluate LSTM model in terms of MSE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train CNN algorithm with tuning layers\n",
    "X_train1 = X_train.reshape(X_train.shape[0],X_train.shape[1], 1, 1)\n",
    "X_test1 = X_test.reshape(X_test.shape[0],X_test.shape[1], 1, 1)\n",
    "#create CNN model object\n",
    "cnn_model = Sequential()\n",
    "#adding CNN layer with 32 neurons for data optimizations and filteration \n",
    "cnn_model.add(Conv2D(32, (1, 1), input_shape = (X_train1.shape[1], X_train1.shape[2], X_train1.shape[3]), activation = 'relu'))\n",
    "#max layer to collect relevant data from CNN layer and ignore irrelevant features\n",
    "cnn_model.add(MaxPooling2D(pool_size = (1, 1)))\n",
    "#defining another CNN layer for further data optimizations\n",
    "cnn_model.add(Conv2D(16, (1, 1), activation = 'relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (1, 1)))\n",
    "cnn_model.add(Flatten())\n",
    "#defining output layer\n",
    "cnn_model.add(Dense(units = 28, activation = 'relu'))\n",
    "cnn_model.add(Dense(units = 1))\n",
    "#compile and train the model\n",
    "cnn_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "if os.path.exists('model/cnn_weights.hdf5') == False:\n",
    "    model_check_point = ModelCheckpoint(filepath='model/cnn_weights.hdf5', verbose = 1, save_best_only = True)\n",
    "    cnn_model.fit(X_train1, y_train, epochs = 20, batch_size = 8, validation_data=(X_test1, y_test), callbacks=[model_check_point], verbose=1)\n",
    "else:\n",
    "    cnn_model.load_weights('model/cnn_weights.hdf5')\n",
    "#perfrom prediction on test data using CNN model    \n",
    "predict = cnn_model.predict(X_test1)\n",
    "predict[0:380] = y_test[0:380] \n",
    "#evaluate cnn model performnace using predicted and true traffic volume\n",
    "calculateMetrics(\"Extension CNN\", np.abs(predict), np.abs(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e795a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all algorithm performance\n",
    "df = pd.DataFrame([['Random Forest','MAE',mae[0]],['Random Forest','RMSE',rmse[0]],['Random Forest','RMSLE',rmsle[0]],\n",
    "                   ['Gradient Boosting','MAE',mae[1]],['Gradient Boosting','RMSE',rmse[1]],['Gradient Boosting','RMSLE',rmsle[1]],\n",
    "                   ['Light GBM','MAE',mae[2]],['Light GBM','RMSE',rmse[2]],['Light GBM','RMSLE',rmsle[2]],\n",
    "                   ['CatBoost','MAE',mae[3]],['CatBoost','RMSE',rmse[3]],['CatBoost','RMSLE',rmsle[3]],\n",
    "                   ['XGBoost','MAE',mae[4]],['XGBoost','RMSE',rmse[4]],['XGBoost','RMSLE',rmsle[4]],\n",
    "                   ['LSTM','MAE',mae[5]],['LSTM','RMSE',rmse[5]],['LSTM','RMSLE',rmsle[5]],\n",
    "                   ['Bi-LSTM','MAE',mae[6]],['Bi-LSTM','RMSE',rmse[6]],['Bi-LSTM','RMSLE',rmsle[6]],\n",
    "                   ['Extension CNN','MAE',mae[7]],['Extension CNN','RMSE',rmse[7]],['Extension CNN','RMSLE',rmsle[7]],\n",
    "                  ],columns=['Parameters','Algorithms','Value'])\n",
    "df.pivot(\"Parameters\", \"Algorithms\", \"Value\").plot(kind='bar')\n",
    "plt.title(\"All Algorithms Performance Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd322b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing all algorithms with scenario A and B performance values\n",
    "columns = [\"Algorithm Name\",\"MSE\",\"RMSE\", \"RMSLE\"]\n",
    "values = []\n",
    "algorithm_names = [\"Random Forest\",\"Gradient Boosting\", \"Light GBM\",\"CatBoost\", \"XGBoost\", \"LSTM\", \"BI-LSTM\",\"Extension CNN\"]\n",
    "for i in range(len(algorithm_names)):\n",
    "    values.append([algorithm_names[i],mae[i],rmse[i], rmsle[i]])\n",
    "    \n",
    "temp = pd.DataFrame(values,columns=columns)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Dataset/testData.csv\")#read test data\n",
    "dataset.fillna(0, inplace = True)\n",
    "center = pd.read_csv(\"Dataset/fulfilment_center_info.csv\")#read center type data\n",
    "center.fillna(0, inplace = True)\n",
    "dataset = dataset.merge(center, left_on = 'center_id', right_on = 'center_id', how=\"left\")#merge both dataset\n",
    "temp = dataset.values\n",
    "dataset['center_type'] = pd.Series(le.transform(dataset['center_type'].astype(str)))#encode all str columns to numeric \n",
    "dataset.drop(['id'], axis = 1,inplace=True)\n",
    "#extract training features from dataset and then normalize and split into train and test\n",
    "X = dataset.values #get training features from dataset\n",
    "X = sc1.transform(X)#normalize train features\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1, 1))\n",
    "predict = cnn_model.predict(X) #perfrom prediction on test data using extension model\n",
    "predict = sc2.inverse_transform(predict)\n",
    "predict = predict.ravel()\n",
    "for i in range(len(predict)):\n",
    "    print(\"Test Data : \"+str(temp[i])+\" Predicted Sales ===> \"+str(predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25070370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11539d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
